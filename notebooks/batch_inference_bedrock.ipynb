{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch inference with Bedrock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook aims to provide an example on how to run batch inference with Bedrock. \n",
    "\n",
    "With batch inference, you can process a larger number of prompts in a more efficient way. Also, according to [pricing](https://aws.amazon.com/bedrock/pricing/), it is ~50% cheaper than doing inference with single requests.\n",
    "\n",
    "Note: Not all the models support batch inference. Check [here](https://docs.aws.amazon.com/bedrock/latest/userguide/batch-inference-supported.html) the ones that do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we need to create a .jsonl dataset with the following format:\n",
    "\n",
    "```{ \"recordId\" : \"00001\", \"modelInput\" : {JSON body} }```\n",
    "\n",
    "The format of the modelInput JSON object must match the body field for the model that you use in the InvokeModel request."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example, we will use an Anthropic Claude model. Let's create some data with the required format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TOKENS = 1000\n",
    "SYSTEM_PROMPT = \"Please respond only with emoji.\"\n",
    "\n",
    "user_message = {\"role\": \"user\", \"content\": \"Hello there! How are you doing today?\"}\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"recordId\": \"CALL0000001\",\n",
    "        \"modelInput\": {\n",
    "            \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "            \"max_tokens\": MAX_TOKENS,\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": \"Hello there! How are you doing today?\"}],\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"recordId\": \"CALL0000002\",\n",
    "        \"modelInput\": {\n",
    "            \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "            \"max_tokens\": MAX_TOKENS,\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": \"My cat is so cute!\"}],\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"recordId\": \"CALL0000003\",\n",
    "        \"modelInput\": {\n",
    "            \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "            \"max_tokens\": MAX_TOKENS,\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": \"I'm programming today\"}],\n",
    "        },\n",
    "    },\n",
    "]\n",
    "\n",
    "examples = 100 * examples  # there is a requirement in Bedrock for having min 100 input samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/anavillalba/Code/mvp-mlops-platform/.venv/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: jsonlines in /Users/anavillalba/Code/mvp-mlops-platform/.venv/lib/python3.10/site-packages (4.0.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /Users/anavillalba/Code/mvp-mlops-platform/.venv/lib/python3.10/site-packages (from jsonlines) (24.2.0)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/anavillalba/Code/mvp-mlops-platform/.venv/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/anavillalba/Code/mvp-mlops-platform/.venv/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install jsonlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "\n",
    "with jsonlines.open(\"examples.jsonl\", mode=\"w\") as writer:\n",
    "    writer.write_all(examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload the dataset to S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way batch inference works with Bedrock is as follows:\n",
    "- You upload (or already have) a dataset with your prompts in an S3 bucket.\n",
    "- You run the batch inference job. \n",
    "- The results are written in an S3 bucket. Then, if needed, you can download them.\n",
    "\n",
    "Thus, the next step is to upload the jsonl file with the data to S3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to do so, we need to be authenticated in our AWS ml account:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "ml_session = boto3.Session(profile_name=\"ml\", region_name=\"us-east-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ml-rd-bedrock-datasets\n",
      "ml-rd-bedrock-inference-outputs\n",
      "ml-rd-mlflow-artifact-storage\n",
      "ml-rd-query-rewriting-experiments\n",
      "sagemaker-studio-43ifgnfx4ho\n",
      "sagemaker-studio-ezt7qs2smmu\n",
      "sagemaker-us-east-1-879381254630\n"
     ]
    }
   ],
   "source": [
    "s3_client = ml_session.client(\"s3\")\n",
    "\n",
    "response = s3_client.list_buckets()\n",
    "\n",
    "for bucket in response[\"Buckets\"]:\n",
    "    print(bucket[\"Name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The S3 bucket ```ml-rd-bedrock-datasets``` has been created as the storage for Bedrock datasets, so we will use it in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from botocore.exceptions import ClientError\n",
    "\n",
    "try:\n",
    "    s3_client.upload_file(\"examples.jsonl\", \"ml-rd-bedrock-datasets\", \"examples.jsonl\")\n",
    "except ClientError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if you check the S3 bucket, you'll our dataset stored as examples.jsonl."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run inference batch job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's run the inference batch job in Bedrock. To keep things organized, we will create 1 folder per job to store its outputs.\n",
    "\n",
    "We will use the bucket ```ml-rd-bedrock-inference-outputs```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now define the input and output S3 buckets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_config = {\"s3InputDataConfig\": {\"s3Uri\": \"s3://ml-rd-bedrock-datasets/examples.jsonl\"}}\n",
    "\n",
    "output_data_config = {\"s3OutputDataConfig\": {\"s3Uri\": \"s3://ml-rd-bedrock-inference-outputs\"}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, let's run the batch job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully launched job with ARN: arn:aws:bedrock:us-east-1:879381254630:model-invocation-job/i6kuqtep06cn\n"
     ]
    }
   ],
   "source": [
    "bedrock_client = ml_session.client(\"bedrock\")\n",
    "\n",
    "response = bedrock_client.create_model_invocation_job(\n",
    "    roleArn=ml_session.client(\"iam\").get_role(\n",
    "        RoleName=\"AmazonSageMaker-ExecutionRole-20241203T102031\"\n",
    "    )[\"Role\"][\"Arn\"],\n",
    "    modelId=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "    jobName=f\"example-bedrock-batch-inference-job-v8\",\n",
    "    inputDataConfig=input_data_config,\n",
    "    outputDataConfig=output_data_config,\n",
    ")\n",
    "\n",
    "job_arn = response[\"jobArn\"]\n",
    "print(f\"Successfully launched job with ARN: {job_arn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Scheduled'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bedrock_client.get_model_invocation_job(jobIdentifier=job_arn)[\"status\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can go to the Bedrock UI and check your job under Inference and Assessment > Batch inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Download the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optionally, we can download the results from S3 to our local filesystem. Of course, this step is not recommended if the dataset is huge, as it might take lots of time and storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_id = job_arn.split(\"/\")[\n",
    "    1\n",
    "]  # results are stored in s3://ml-rd-bedrock-inference-outputs/{job_id}/examples.jsonl.out\n",
    "\n",
    "s3_client.download_file(\n",
    "    \"ml-rd-bedrock-inference-outputs\", f\"{job_id}/examples.jsonl.out\", \"examples.jsonl.out\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More resources\n",
    "\n",
    "- [Another example from AWS](https://github.com/aws-samples/amazon-bedrock-samples/blob/main/introduction-to-bedrock/batch_api/batch-inference-transcript-summarization.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
