{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWS Bedrock Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook aims to provide an example of how to access [Bedrock](https://docs.aws.amazon.com/bedrock/latest/userguide/what-is-bedrock.html) and call one of their hosted LLMs.\n",
    "\n",
    "The LLM of this example is an Anthropic Claude model, and we are going to simulate a virtual assistant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-requisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to interact with AWS Bedrock, you need to configure authentication first. \n",
    "\n",
    "Follow [these instructions](https://github.com/elastic/cloud/blob/master/wiki/AWS.md#configuring-okta-awscli-for-cli--api-access) in order to do so. You will need a Yubikey to enable MFA, Touch ID in MacOS no longer works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we need to do is to create the Bedrock runtime client. To do so, we need to authenticate ourselves in the ML account:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "ml_session = boto3.Session(profile_name=\"ml\", region_name=\"us-east-1\")\n",
    "bedrock_runtime_client = ml_session.client(\"bedrock-runtime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define now some initial parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "system_prompt = \"Please respond only with emoji.\"\n",
    "max_tokens = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's invoke the model with an initial prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_message = {\"role\": \"user\", \"content\": \"Hello there! How are you doing today?\"}\n",
    "messages = [user_message]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'msg_bdrk_01FSqdzLSLoeVD7c3ucTDy3q', 'type': 'message', 'role': 'assistant', 'model': 'claude-3-sonnet-20240229', 'content': [{'type': 'text', 'text': 'üëãüòä (Waving hello with a smiling face)'}], 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 22, 'output_tokens': 20}}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "try:\n",
    "    body = json.dumps(\n",
    "        {\n",
    "            \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "            \"max_tokens\": max_tokens,\n",
    "            \"system\": system_prompt,\n",
    "            \"messages\": messages,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    response = bedrock_runtime_client.invoke_model(body=body, modelId=model_id)\n",
    "    response_body = json.loads(response[\"body\"].read())\n",
    "    print(response_body)\n",
    "\n",
    "except ClientError as e:\n",
    "    message = e.response[\"Error\"][\"Message\"]\n",
    "    print(f\"A client error occurred: {message}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check the different models and options that you have at hand with Bedrock, check [this chapter](https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters.html) of the documentation.\n",
    "\n",
    "To check the pricing of each inference, see the [pricing information](https://aws.amazon.com/bedrock/pricing/).\n",
    "\n",
    "Check [here](https://docs.aws.amazon.com/bedrock/latest/userguide/models-regions.html) the models supported by region.\n",
    "\n",
    "And, finally, check [here](https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference.html) the Converse API, which is more intended for conversations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Store the result of MLflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use MLflow to track your prompts and responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, connect to our centralized MLflow server:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_sagemaker_session = boto3.Session(\n",
    "    profile_name=\"sagemaker\"\n",
    ")  # you can also use the existing ml session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "credentials = ml_sagemaker_session.get_credentials().get_frozen_credentials()\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = credentials.access_key\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = credentials.secret_key\n",
    "os.environ[\"AWS_SESSION_TOKEN\"] = credentials.token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "mlflow_server_arn = (\n",
    "    \"arn:aws:sagemaker:us-east-1:879381254630:mlflow-tracking-server/ml-rd-mlflow-server\"\n",
    ")\n",
    "\n",
    "mlflow.set_tracking_uri(mlflow_server_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's log the input prompt and the response we got:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='s3://ml-rd-mlflow-artifact-storage/artifacts/35', creation_time=1734091856180, experiment_id='35', last_update_time=1734091856180, lifecycle_stage='active', name='bedrock-example', tags={}>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.set_experiment(\"bedrock-example\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/12/13 13:14:18 INFO mlflow.tracking._tracking_service.client: üèÉ View run bedrock-emoji-example at: https://us-east-1.experiments.sagemaker.aws/#/experiments/35/runs/2cfc68b3bcec44bf9018e19e2603a50d.\n",
      "2024/12/13 13:14:18 INFO mlflow.tracking._tracking_service.client: üß™ View experiment at: https://us-east-1.experiments.sagemaker.aws/#/experiments/35.\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run(run_name=\"bedrock-emoji-example\"):\n",
    "    mlflow.log_text(text=system_prompt, artifact_file=\"system_prompt.txt\")\n",
    "    mlflow.log_text(text=json.dumps(user_message), artifact_file=\"user_message.json\")\n",
    "    mlflow.log_text(text=json.dumps(response_body), artifact_file=\"response.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now go and check the results of the mlflow UI!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
